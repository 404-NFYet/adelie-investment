"""
Plotly ì‹œê°í™” ì½”ë“œ ìƒì„± ëª¨ë¸ ë¹„êµ í…ŒìŠ¤íŠ¸
Claude (claude-3-5-sonnet) vs OpenAI (gpt-4o-mini) vs OpenAI (gpt-4o)

ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ë¡œ 3ê°œ ëª¨ë¸ì˜ Plotly ì½”ë“œ ìƒì„± í’ˆì§ˆì„ ë¹„êµí•œë‹¤.
"""

import os
import sys
import json
import time
import asyncio
import subprocess
import tempfile
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
PROJECT_ROOT = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

from dotenv import load_dotenv
load_dotenv(PROJECT_ROOT / ".env")

# ========================
# í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ (ë””ìì¸ ì‹œìŠ¤í…œ í¬í•¨)
# ========================

SYSTEM_PROMPT = """Python Plotlyë¥¼ ì‚¬ìš©í•˜ì—¬ ì‹œê°í™” ì½”ë“œë¥¼ ì‘ì„±í•˜ì„¸ìš”.

## ë””ìì¸ ê·œì¹™
- ì£¼ ìƒ‰ìƒ: #FF6B00 (ì˜¤ë Œì§€), ë³´ì¡°: #4A90D9 (íŒŒë‘)
- ë°°ê²½: íˆ¬ëª… (paper_bgcolor='rgba(0,0,0,0)', plot_bgcolor='rgba(0,0,0,0)')
- í°íŠ¸: 'IBM Plex Sans KR', size=12, color='#4E5968'
- ê·¸ë¦¬ë“œ: color='#F2F4F6'
- ì¶• ë ˆì´ë¸”: color='#8B95A1', size=11
- ë§ˆì§„: dict(t=50, b=60, l=60, r=30)
- Yì¶•ì— ë°˜ë“œì‹œ ë‹¨ìœ„ ê´„í˜¸ í‘œê¸° (ì˜ˆ: "PER (ë°°)", "ê¸ˆì•¡ (ì–µ ì›)")
- ë°ì´í„° í¬ì¸íŠ¸ì— ê°’ì„ ì§ì ‘ í‘œì‹œ (textposition='outside')
- ê¹”ë”í•˜ê³  ë¯¸ë‹ˆë©€í•œ ìŠ¤íƒ€ì¼, í•œê¸€ ì‚¬ìš©

## ì½”ë“œ ê·œì¹™
- import plotly.graph_objects as go (expressë„ í—ˆìš©)
- import pandas, numpy í—ˆìš©
- fig.write_html('/output/chart.html', include_plotlyjs='cdn', full_html=True)
- ì½”ë“œë§Œ ì¶œë ¥, ì„¤ëª… ì—†ì´"""

TEST_PROMPTS = [
    {
        "id": "bar_comparison",
        "name": "ê³¼ê±°-í˜„ì¬ ë¹„êµ ë§‰ëŒ€ ì°¨íŠ¸",
        "prompt": """2000ë…„ ë‹·ì»´ë²„ë¸”ê³¼ 2026ë…„ AI ë¶ì˜ PER ë¹„êµ ë§‰ëŒ€ ì°¨íŠ¸ë¥¼ ë§Œë“œì„¸ìš”.
ë°ì´í„°:
- ì‹œìŠ¤ì½” (2000ë…„): PER 150ë°°
- ì˜¤ë¼í´ (2000ë…„): PER 100ë°°
- ì—”ë¹„ë””ì•„ (2026ë…„): PER 60ë°°
- MS (2026ë…„): PER 35ë°°
ê³¼ê±°ëŠ” íšŒìƒ‰(#ADB5BD), í˜„ì¬ëŠ” ì˜¤ë Œì§€(#FF6B00)ë¡œ êµ¬ë¶„í•˜ì„¸ìš”.
ê° ë§‰ëŒ€ ìœ„ì— ê°’ì„ "150ë°°" í˜•ì‹ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”.""",
    },
    {
        "id": "trend_line",
        "name": "ì‹œê³„ì—´ ì¶”ì´ ë¼ì¸ ì°¨íŠ¸",
        "prompt": """ê¸€ë¡œë²Œ AI ìë³¸ì§€ì¶œ ì „ë§ ë¼ì¸ ì°¨íŠ¸ë¥¼ ë§Œë“œì„¸ìš”.
ë°ì´í„°:
- 2023: 580ì–µ ë‹¬ëŸ¬
- 2024: 680ì–µ ë‹¬ëŸ¬
- 2025: 750ì–µ ë‹¬ëŸ¬
- 2026(E): 870ì–µ ë‹¬ëŸ¬
- 2027(E): 960ì–µ ë‹¬ëŸ¬
ë¼ì¸ ìƒ‰ìƒì€ #FF6B00, ì˜ì—­ì€ ë°˜íˆ¬ëª… ì˜¤ë Œì§€ë¡œ ì±„ìš°ì„¸ìš”.
ê° í¬ì¸íŠ¸ì— ê°’ì„ í‘œì‹œí•˜ì„¸ìš”. Yì¶•: "ê¸ˆì•¡ (ì–µ ë‹¬ëŸ¬)".""",
    },
    {
        "id": "risk_area",
        "name": "ë¦¬ìŠ¤í¬ ì˜ì—­ ì°¨íŠ¸",
        "prompt": """ë‚˜ìŠ¤ë‹¥ ì§€ìˆ˜ í•˜ë½/íšŒë³µ íŒ¨í„´ ì˜ì—­ ì°¨íŠ¸ë¥¼ ë§Œë“œì„¸ìš”.
ë°ì´í„° (2000=100 ê¸°ì¤€):
- 2000.03: 100
- 2000.09: 65
- 2001.03: 45
- 2001.09: 35
- 2002.10: 25 (ìµœì €ì )
- 2003.06: 40
- 2004.01: 55
ìµœì €ì ì— "ìµœëŒ€ -75% í•˜ë½" ì£¼ì„ì„ ì¶”ê°€í•˜ì„¸ìš”.
í•˜ë½ ì˜ì—­ì€ ì—°í•œ ë¹¨ê°•, íšŒë³µ ì˜ì—­ì€ ì—°í•œ ì´ˆë¡ìœ¼ë¡œ í‘œì‹œí•˜ì„¸ìš”.
Yì¶•: "ì§€ìˆ˜ (2000.03=100)", Xì¶•: "ì‹œì ".""",
    },
]


# ========================
# ëª¨ë¸ í˜¸ì¶œ í•¨ìˆ˜
# ========================

async def call_openai(model: str, system: str, prompt: str) -> dict:
    """OpenAI API í˜¸ì¶œ"""
    from openai import AsyncOpenAI
    client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
    
    start = time.time()
    response = await client.chat.completions.create(
        model=model,
        messages=[
            {"role": "system", "content": system},
            {"role": "user", "content": prompt},
        ],
        max_tokens=2000,
        temperature=0.2,
    )
    elapsed = time.time() - start
    
    code = response.choices[0].message.content
    # ì½”ë“œ ë¸”ë¡ ì œê±°
    if "```python" in code:
        code = code.split("```python")[1].split("```")[0].strip()
    elif "```" in code:
        code = code.split("```")[1].split("```")[0].strip()
    
    return {
        "model": model,
        "code": code,
        "tokens": response.usage.total_tokens if response.usage else 0,
        "latency_s": round(elapsed, 2),
    }


async def call_claude(model: str, system: str, prompt: str) -> dict:
    """Claude API í˜¸ì¶œ"""
    import anthropic
    client = anthropic.AsyncAnthropic(api_key=os.getenv("CLAUDE_API_KEY"))
    
    start = time.time()
    response = await client.messages.create(
        model=model,
        max_tokens=2000,
        system=system,
        messages=[{"role": "user", "content": prompt}],
        temperature=0.2,
    )
    elapsed = time.time() - start
    
    code = response.content[0].text
    # ì½”ë“œ ë¸”ë¡ ì œê±°
    if "```python" in code:
        code = code.split("```python")[1].split("```")[0].strip()
    elif "```" in code:
        code = code.split("```")[1].split("```")[0].strip()
    
    input_tokens = response.usage.input_tokens if response.usage else 0
    output_tokens = response.usage.output_tokens if response.usage else 0
    
    return {
        "model": model,
        "code": code,
        "tokens": input_tokens + output_tokens,
        "latency_s": round(elapsed, 2),
    }


# ========================
# ì½”ë“œ ì‹¤í–‰ ë° í‰ê°€
# ========================

def execute_plotly_code(code: str, output_dir: str) -> dict:
    """Plotly ì½”ë“œë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ í™•ì¸"""
    output_path = os.path.join(output_dir, "chart.html")
    os.makedirs(os.path.join(output_dir), exist_ok=True)
    
    # /output/ ê²½ë¡œë¥¼ ì‹¤ì œ ê²½ë¡œë¡œ ì¹˜í™˜
    modified_code = code.replace("/output/chart.html", output_path)
    modified_code = modified_code.replace("'/output/", f"'{output_dir}/")
    
    # ì„ì‹œ íŒŒì¼ì— ì½”ë“œ ì‘ì„±
    script_path = os.path.join(output_dir, "script.py")
    with open(script_path, "w") as f:
        f.write(modified_code)
    
    # ì‹¤í–‰
    start = time.time()
    try:
        result = subprocess.run(
            [sys.executable, script_path],
            capture_output=True, text=True, timeout=30,
            cwd=output_dir,
        )
        elapsed = time.time() - start
        
        success = result.returncode == 0 and os.path.exists(output_path)
        html_size = os.path.getsize(output_path) if success else 0
        
        return {
            "success": success,
            "execution_time_s": round(elapsed, 2),
            "html_size_kb": round(html_size / 1024, 1) if success else 0,
            "html_path": output_path if success else None,
            "error": result.stderr[:500] if not success else None,
        }
    except subprocess.TimeoutExpired:
        return {"success": False, "error": "Timeout (30s)", "execution_time_s": 30}
    except Exception as e:
        return {"success": False, "error": str(e)[:500]}


def evaluate_code_quality(code: str) -> dict:
    """ì½”ë“œ í’ˆì§ˆ í‰ê°€ (ì •ì  ë¶„ì„)"""
    checks = {
        "has_write_html": "write_html" in code,
        "has_transparent_bg": "rgba(0,0,0,0)" in code or "transparent" in code.lower(),
        "has_korean_font": "IBM Plex" in code or "Noto Sans" in code or "font" in code.lower(),
        "has_yaxis_unit": any(u in code for u in ["(ë°°)", "(ì–µ", "(%)", "(ì›)", "(ë‹¬ëŸ¬)", "(í¬ì¸íŠ¸)"]),
        "has_text_on_data": "text=" in code or "textposition" in code,
        "has_color_ff6b00": "#FF6B00" in code or "#ff6b00" in code,
        "has_margin": "margin" in code,
        "line_count": code.count("\n") + 1,
    }
    
    score = sum([
        checks["has_write_html"] * 2,
        checks["has_transparent_bg"] * 1,
        checks["has_korean_font"] * 1,
        checks["has_yaxis_unit"] * 2,
        checks["has_text_on_data"] * 2,
        checks["has_color_ff6b00"] * 1,
        checks["has_margin"] * 1,
    ])
    
    return {"checks": checks, "quality_score": score, "max_score": 10}


# ========================
# ë©”ì¸ í…ŒìŠ¤íŠ¸
# ========================

async def run_tests():
    """ëª¨ë“  ëª¨ë¸ Ã— ëª¨ë“  í”„ë¡¬í”„íŠ¸ ì¡°í•© í…ŒìŠ¤íŠ¸"""
    
    models = [
        ("openai", "gpt-4o-mini"),
        ("openai", "gpt-4o"),
        ("claude", "claude-3-5-sonnet-20241022"),
        ("claude", "claude-3-5-haiku-20241022"),
        ("claude", "claude-sonnet-4-20250514"),
    ]
    
    results = []
    output_base = os.path.join(PROJECT_ROOT, "tests", "viz_test_output")
    os.makedirs(output_base, exist_ok=True)
    
    for provider, model in models:
        for test in TEST_PROMPTS:
            print(f"\n{'='*60}")
            print(f"ëª¨ë¸: {model} | í…ŒìŠ¤íŠ¸: {test['name']}")
            print(f"{'='*60}")
            
            # 1. ì½”ë“œ ìƒì„±
            try:
                if provider == "openai":
                    gen_result = await call_openai(model, SYSTEM_PROMPT, test["prompt"])
                else:
                    gen_result = await call_claude(model, SYSTEM_PROMPT, test["prompt"])
            except Exception as e:
                print(f"  âŒ API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
                results.append({
                    "model": model, "test": test["id"], "test_name": test["name"],
                    "api_error": str(e),
                })
                continue
            
            print(f"  ìƒì„± ì‹œê°„: {gen_result['latency_s']}s | í† í°: {gen_result['tokens']}")
            
            # 2. ì½”ë“œ í’ˆì§ˆ í‰ê°€
            quality = evaluate_code_quality(gen_result["code"])
            print(f"  í’ˆì§ˆ ì ìˆ˜: {quality['quality_score']}/{quality['max_score']}")
            for k, v in quality["checks"].items():
                if k != "line_count":
                    status = "âœ…" if v else "âŒ"
                    print(f"    {status} {k}")
            
            # 3. ì½”ë“œ ì‹¤í–‰
            output_dir = os.path.join(output_base, f"{model.replace('/', '_')}_{test['id']}")
            exec_result = execute_plotly_code(gen_result["code"], output_dir)
            
            if exec_result["success"]:
                print(f"  âœ… ì‹¤í–‰ ì„±ê³µ: {exec_result['execution_time_s']}s | HTML: {exec_result['html_size_kb']}KB")
                print(f"  ğŸ“„ íŒŒì¼: {exec_result['html_path']}")
            else:
                print(f"  âŒ ì‹¤í–‰ ì‹¤íŒ¨: {exec_result.get('error', 'Unknown')[:200]}")
            
            # ê²°ê³¼ ì €ì¥
            results.append({
                "model": model,
                "test": test["id"],
                "test_name": test["name"],
                "latency_s": gen_result["latency_s"],
                "tokens": gen_result["tokens"],
                "quality_score": quality["quality_score"],
                "quality_max": quality["max_score"],
                "checks": quality["checks"],
                "exec_success": exec_result["success"],
                "exec_time_s": exec_result.get("execution_time_s"),
                "html_size_kb": exec_result.get("html_size_kb"),
                "html_path": exec_result.get("html_path"),
                "exec_error": exec_result.get("error"),
                "code": gen_result["code"],
            })
    
    # ========================
    # ìš”ì•½ ë¦¬í¬íŠ¸
    # ========================
    print("\n\n" + "=" * 80)
    print("ğŸ“Š ëª¨ë¸ ë¹„êµ ìš”ì•½ ë¦¬í¬íŠ¸")
    print("=" * 80)
    
    # ëª¨ë¸ë³„ ì§‘ê³„
    model_summary = {}
    for r in results:
        m = r["model"]
        if m not in model_summary:
            model_summary[m] = {
                "total": 0, "exec_success": 0, 
                "total_quality": 0, "total_latency": 0, "total_tokens": 0,
            }
        model_summary[m]["total"] += 1
        model_summary[m]["exec_success"] += 1 if r.get("exec_success") else 0
        model_summary[m]["total_quality"] += r.get("quality_score", 0)
        model_summary[m]["total_latency"] += r.get("latency_s", 0)
        model_summary[m]["total_tokens"] += r.get("tokens", 0)
    
    print(f"\n{'ëª¨ë¸':<35} {'ì‹¤í–‰ì„±ê³µ':<10} {'í’ˆì§ˆí‰ê· ':<10} {'ì‘ë‹µì‹œê°„':<10} {'í† í°í•©ê³„':<10}")
    print("-" * 75)
    for m, s in model_summary.items():
        avg_quality = s["total_quality"] / max(s["total"], 1)
        avg_latency = s["total_latency"] / max(s["total"], 1)
        print(f"{m:<35} {s['exec_success']}/{s['total']:<8} {avg_quality:.1f}/10    {avg_latency:.1f}s      {s['total_tokens']}")
    
    # ê²°ê³¼ JSON ì €ì¥
    report_path = os.path.join(output_base, "comparison_report.json")
    with open(report_path, "w") as f:
        json.dump(results, f, ensure_ascii=False, indent=2, default=str)
    print(f"\nìƒì„¸ ë¦¬í¬íŠ¸: {report_path}")
    
    # ìƒì„±ëœ HTML íŒŒì¼ ëª©ë¡
    print("\nğŸ“„ ìƒì„±ëœ ì‹œê°í™” íŒŒì¼:")
    for r in results:
        if r.get("html_path"):
            print(f"  {r['model']} / {r['test_name']}: {r['html_path']}")
    
    return results


if __name__ == "__main__":
    asyncio.run(run_tests())
