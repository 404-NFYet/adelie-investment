"""í†µí•© ë°ì´í„° íŒŒì´í”„ë¼ì¸: Phase 1-4 ì—°ì† ì‹¤í–‰.

ë©€í‹°ë°ì´ íŠ¸ë Œë“œ ê°ì§€ â†’ ì„¹í„° í´ëŸ¬ìŠ¤í„°ë§ â†’ RSS ë‰´ìŠ¤ ë§¤ì¹­ â†’ í…Œë§ˆ í‚¤ì›Œë“œ ìƒì„±ì„
í•˜ë‚˜ì˜ íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì‹¤í–‰í•˜ì—¬ ë°ì´í„° ì¼ê´€ì„± ë³´ì¥.

ì˜ˆì™¸ ì²˜ë¦¬:
- íœ´ì¼ ìë™ ê°ì§€ ë° fallback
- API ì¬ì‹œë„ ë¡œì§ (exponential backoff)
- ìµœì†Œ ë°ì´í„° ë³´ì¥ (í‚¤ì›Œë“œ 3ê°œ, ì¢…ëª© 5ê°œ)
"""
import asyncio
import json
import os
import re
import sys
import time
from collections import defaultdict
from datetime import datetime, timedelta, timezone
from difflib import SequenceMatcher
from email.utils import parsedate_to_datetime
from pathlib import Path

import httpx
import pandas as pd
from pykrx import stock as pykrx_stock

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))


# ============================================================
# ì¬ì‹œë„ ë°ì½”ë ˆì´í„°
# ============================================================

def retry_with_backoff(max_attempts=3, base_delay=2):
    """Exponential backoff ì¬ì‹œë„ ë°ì½”ë ˆì´í„°."""

    def decorator(func):
        async def async_wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_attempts - 1:
                        print(f"  âŒ {func.__name__} ìµœì¢… ì‹¤íŒ¨: {e}")
                        raise
                    delay = base_delay * (2**attempt)
                    print(f"  âš ï¸  {func.__name__} ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_attempts}), {delay}ì´ˆ í›„ ì¬ì‹œë„...")
                    await asyncio.sleep(delay)

        def sync_wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_attempts - 1:
                        print(f"  âŒ {func.__name__} ìµœì¢… ì‹¤íŒ¨: {e}")
                        raise
                    delay = base_delay * (2**attempt)
                    print(f"  âš ï¸  {func.__name__} ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{max_attempts}), {delay}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(delay)

        return async_wrapper if asyncio.iscoroutinefunction(func) else sync_wrapper

    return decorator


# ============================================================
# ìƒìˆ˜ ë° ì„¤ì •
# ============================================================

RSS_FEEDS = [
    "https://www.hankyung.com/feed/economy",
]

_SECTOR_MIRRORING_HINTS = {
    "ë°˜ë„ì²´": "2018ë…„ ë©”ëª¨ë¦¬ ë‹¤ìš´ì‚¬ì´í´: ì‚¼ì„±ì „ìÂ·SKí•˜ì´ë‹‰ìŠ¤ ì£¼ê°€ 40%+ í•˜ë½ í›„ 2019ë…„ ë°˜ë“±",
    "2ì°¨ì „ì§€": "2021ë…„ ë°°í„°ë¦¬ì£¼ ê¸‰ë“±ë½: LGì—ë„ˆì§€ì†”ë£¨ì…˜ IPO ì „í›„ ì—ì½”í”„ë¡œÂ·ì—˜ì•¤ì—í”„ 300%+ ìƒìŠ¹ í›„ 2022ë…„ 50% ì¡°ì •",
    "ìë™ì°¨": "2020ë…„ í˜„ëŒ€ì°¨ ì „ê¸°ì°¨ ì „ëµ ë°œí‘œ: ì£¼ê°€ 2ë°° ìƒìŠ¹ í›„ 2021ë…„ ì¡°ì •, ì•„ì´ì˜¤ë‹‰5 ì¶œì‹œ íš¨ê³¼",
    "ë°”ì´ì˜¤": "2020ë…„ ì…€íŠ¸ë¦¬ì˜¨Â·ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤ ì½”ë¡œë‚˜ ìˆ˜í˜œ ê¸‰ë“±, 2021ë…„ ì„ìƒ ì‹¤íŒ¨ ë¦¬ìŠ¤í¬ë¡œ 30% ì¡°ì •",
    "ê±´ì„¤": "2015ë…„ í•´ì™¸ê±´ì„¤ ìˆ˜ì£¼ ê¸‰ê°: ëŒ€ìš°ê±´ì„¤Â·í˜„ëŒ€ê±´ì„¤ ì£¼ê°€ 30% í•˜ë½, ë¶€ë™ì‚° PF ë¦¬ìŠ¤í¬ ë¶€ê°",
    "ê¸ˆìœµ": "2008ë…„ ê¸€ë¡œë²Œ ê¸ˆìœµìœ„ê¸°: KBê¸ˆìœµÂ·ì‹ í•œì§€ì£¼ 50% ê¸‰ë½ í›„ 2009ë…„ ì •ë¶€ ì§€ì›ìœ¼ë¡œ 80% ë°˜ë“±",
    "IT ì„œë¹„ìŠ¤": "2021ë…„ ì¹´ì¹´ì˜¤Â·ë„¤ì´ë²„ í”Œë«í¼ í™•ì¥: ì¹´ì¹´ì˜¤ë±…í¬Â·ì¹´ì¹´ì˜¤í˜ì´ ìƒì¥, ì£¼ê°€ 2ë°°",
    "ì „ê¸°Â·ì „ì": "2020ë…„ ì‚¼ì„±ì „ìÂ·LGì „ì ê°€ì „ í˜¸í™©: ì¬íƒê·¼ë¬´ ìˆ˜ìš” ê¸‰ì¦, ì£¼ê°€ 50% ìƒìŠ¹",
    "ê¸°ê³„Â·ì¥ë¹„": "2021ë…„ ë‘ì‚°ì¤‘ê³µì—…Â·HDí˜„ëŒ€ì¸í”„ë¼ì½”ì–´ ìˆ˜ì£¼ ì¦ê°€: í’ë ¥Â·ê±´ì„¤ê¸°ê³„ í˜¸í™©",
    "í™”í•™": "2021ë…„ LGí™”í•™Â·SKC ë°°í„°ë¦¬ ì†Œì¬ í˜¸í™©: ì–‘ê·¹ì¬Â·ìŒê·¹ì¬ ê°€ê²© 2ë°°, ì—ì½”í”„ë¡œë¹„ì—  ê¸‰ë“±",
}


# ============================================================
# RSSService (ì„ë² ë“œ)
# ============================================================

class RSSService:
    """RSS í”¼ë“œ ìˆ˜ì§‘ ì„œë¹„ìŠ¤."""

    def __init__(self, feeds: list[str], timeout_seconds: int = 15):
        self.feeds = feeds
        self.timeout_seconds = timeout_seconds

    def fetch_top_news_structured(self, retry_48h: bool = False) -> list[dict]:
        """êµ¬ì¡°í™”ëœ ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜."""
        now = datetime.now(timezone.utc)
        window = timedelta(hours=48 if retry_48h else 24)
        cutoff = now - window
        news_items = []

        with httpx.Client(timeout=self.timeout_seconds, follow_redirects=True) as client:
            for url in self.feeds:
                try:
                    response = client.get(url)
                    if response.status_code >= 400:
                        continue
                    xml = response.text
                    source = self._extract_source(url)
                    self._extract_items(xml, cutoff, news_items, source)
                except Exception:
                    pass

        if len(news_items) < 3 and not retry_48h:
            return self.fetch_top_news_structured(retry_48h=True)

        return news_items[:50]

    def _extract_items(self, xml: str, cutoff: datetime, collector: list, source: str):
        """RSS ì•„ì´í…œ ì¶”ì¶œ."""
        pattern = re.compile(
            r"<item>[\s\S]*?<title>(.*?)</title>"
            r"[\s\S]*?<link>(.*?)</link>"
            r"(?:[\s\S]*?<description>(.*?)</description>)?"
            r"[\s\S]*?(?:<pubDate>(.*?)</pubDate>)?[\s\S]*?</item>",
            re.IGNORECASE,
        )

        for match in pattern.finditer(xml):
            if len(collector) >= 20:
                break
            title = self._clean(match.group(1))
            url = self._clean(match.group(2))
            desc = self._clean(match.group(3)) if match.group(3) else ""
            date_str = match.group(4)

            if not title or not url:
                continue
            if not self._is_recent(date_str, cutoff):
                continue

            collector.append({
                "title": title,
                "url": url,
                "description": desc[:200],
                "published_at": self._parse_date(date_str),
                "source": source,
            })

    @staticmethod
    def _clean(raw: str) -> str:
        """HTML íƒœê·¸ ì œê±°."""
        text = re.sub(r"<!\[CDATA\[(.*?)\]\]>", r"\1", raw or "")
        text = re.sub(r"<[^>]*>", "", text)
        return re.sub(r"\s+", " ", text).strip()

    @staticmethod
    def _extract_source(url: str) -> str:
        """URLì—ì„œ ì¶œì²˜ ì¶”ì¶œ."""
        if "hankyung.com" in url:
            return "í•œêµ­ê²½ì œ"
        return "ê¸°íƒ€"

    @staticmethod
    def _parse_date(date_str: str | None) -> str:
        """ë‚ ì§œ ë¬¸ìì—´ì„ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜."""
        if not date_str:
            return datetime.now(timezone.utc).isoformat()
        try:
            dt = parsedate_to_datetime(date_str.strip())
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            return dt.isoformat()
        except:
            return datetime.now(timezone.utc).isoformat()

    @staticmethod
    def _is_recent(date_str: str | None, cutoff: datetime) -> bool:
        """ìµœê·¼ ë‰´ìŠ¤ì¸ì§€ í™•ì¸."""
        if not date_str:
            return True
        try:
            dt = parsedate_to_datetime(date_str.strip())
            if dt.tzinfo is None:
                dt = dt.replace(tzinfo=timezone.utc)
            return dt >= cutoff
        except:
            return True


# ============================================================
# ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° (RSI, MACD) - pandas-ta ë¶ˆí•„ìš”
# ============================================================


def calculate_rsi(closes: pd.Series, period: int = 14) -> float | None:
    """RSI (Relative Strength Index) ê³„ì‚°.

    RSI > 70: ê³¼ë§¤ìˆ˜ (í•˜ë½ ë¦¬ìŠ¤í¬)
    RSI < 30: ê³¼ë§¤ë„ (ë°˜ë“± ê¸°íšŒ)
    """
    if len(closes) < period + 1:
        return None
    delta = closes.diff()
    gain = delta.where(delta > 0, 0).rolling(window=period).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
    if loss.iloc[-1] == 0:
        return 100.0
    rs = gain.iloc[-1] / loss.iloc[-1]
    return round(100 - (100 / (1 + rs)), 2)


def calculate_macd(closes: pd.Series, fast: int = 12, slow: int = 26, signal: int = 9) -> dict | None:
    """MACD (Moving Average Convergence Divergence) ê³„ì‚°.

    MACD ê³¨ë“ í¬ë¡œìŠ¤ (MACD > Signal): ìƒìŠ¹ ëª¨ë©˜í…€
    MACD ë°ë“œí¬ë¡œìŠ¤ (MACD < Signal): í•˜ë½ ëª¨ë©˜í…€
    """
    if len(closes) < slow + signal:
        return None
    ema_fast = closes.ewm(span=fast, adjust=False).mean()
    ema_slow = closes.ewm(span=slow, adjust=False).mean()
    macd_line = ema_fast - ema_slow
    signal_line = macd_line.ewm(span=signal, adjust=False).mean()
    histogram = macd_line - signal_line
    return {
        "macd": round(float(macd_line.iloc[-1]), 4),
        "signal": round(float(signal_line.iloc[-1]), 4),
        "histogram": round(float(histogram.iloc[-1]), 4),
        "crossover": "bullish" if macd_line.iloc[-1] > signal_line.iloc[-1] else "bearish",
    }


def calculate_technical_indicators(stock_codes: list[str], end_date_str: str) -> dict:
    """ì„ íƒëœ ì¢…ëª©ë“¤ì˜ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°.

    30ì¼ OHLCV ë°ì´í„°ë¥¼ ì¡°íšŒí•˜ì—¬ RSI/MACDë¥¼ ê³„ì‚°í•œë‹¤.
    """
    end_date = datetime.strptime(end_date_str, "%Y%m%d")
    start_date = (end_date - timedelta(days=45)).strftime("%Y%m%d")  # ì¶©ë¶„í•œ ê¸°ê°„
    results = {}

    for code in stock_codes:
        try:
            df = pykrx_stock.get_market_ohlcv_by_date(start_date, end_date_str, code)
            if df is None or len(df) < 14:
                continue

            closes = df["ì¢…ê°€"]
            rsi = calculate_rsi(closes)
            macd = calculate_macd(closes)

            results[code] = {
                "rsi": rsi,
                "macd": macd,
                "macd_signal": macd["crossover"] if macd else None,
            }
        except Exception:
            continue

    return results


# ============================================================
# Sector Rotation ë¡œì§
# ============================================================

SECTOR_ROTATION_MAP = {
    "í™•ì¥ê¸°": ["IT ì„œë¹„ìŠ¤", "ì „ê¸°Â·ì „ì", "ì†Œí”„íŠ¸ì›¨ì–´", "ë°˜ë„ì²´", "ë¯¸ë””ì–´Â·êµìœ¡"],
    "ì •ì ": ["ì—ë„ˆì§€", "í™”í•™", "ì² ê°•Â·ê¸ˆì†", "ì¡°ì„ "],
    "ìˆ˜ì¶•ê¸°": ["ìŒì‹ë£ŒÂ·ë‹´ë°°", "ì˜ì•½í’ˆ", "ìœ í‹¸ë¦¬í‹°", "í†µì‹ "],
    "ì €ì ": ["ê¸ˆìœµ", "ë¶€ë™ì‚°", "ê±´ì„¤", "ë³´í—˜"],
}


def determine_economic_cycle(macro_context: dict) -> str:
    """ê±°ì‹œê²½ì œ ë¶„ì„ ê²°ê³¼ë¡œ ê²½ê¸° ì‚¬ì´í´ íŒë‹¨."""
    analysis = macro_context.get("analysis", "")
    if not analysis:
        return "í™•ì¥ê¸°"  # ê¸°ë³¸ê°’

    # í‚¤ì›Œë“œ ê¸°ë°˜ íŒë‹¨ (í–¥í›„ LLM í™œìš© ê°€ëŠ¥)
    tightening = sum(1 for kw in ["ê¸ˆë¦¬ ì¸ìƒ", "ê¸´ì¶•", "í…Œì´í¼ë§", "ë§¤íŒŒ"] if kw in analysis)
    easing = sum(1 for kw in ["ê¸ˆë¦¬ ì¸í•˜", "ì™„í™”", "ë¹„ë‘˜ê¸°", "ì–‘ì ì™„í™”"] if kw in analysis)
    growth = sum(1 for kw in ["í™•ì¥", "ì„±ì¥", "í˜¸í™©", "ìˆ˜ì¶œ ì¦ê°€"] if kw in analysis)
    contraction = sum(1 for kw in ["ìˆ˜ì¶•", "ë‘”í™”", "ì¹¨ì²´", "ê²½ê¸° í›„í‡´"] if kw in analysis)

    if tightening > easing and growth > contraction:
        return "ì •ì "
    elif easing > tightening:
        return "ì €ì "
    elif growth > contraction:
        return "í™•ì¥ê¸°"
    else:
        return "ìˆ˜ì¶•ê¸°"


def apply_sector_rotation_boost(stocks: list[dict], cycle_phase: str) -> list[dict]:
    """ê²½ê¸° ì‚¬ì´í´ë³„ ì„¹í„° ê°€ì¤‘ì¹˜ ì¡°ì •.

    ìœ ë¦¬í•œ ì„¹í„°: 1.15ë°° ê°€ì‚°
    ë¶ˆë¦¬í•œ ì„¹í„°: ë³€ë™ ì—†ìŒ (1.0ë°°)
    """
    favored_sectors = SECTOR_ROTATION_MAP.get(cycle_phase, [])

    for stock in stocks:
        sector = stock.get("sector", "")
        if any(fav in sector for fav in favored_sectors):
            stock["rotation_boost"] = 1.15
        else:
            stock["rotation_boost"] = 1.0

    return stocks


# ============================================================
# Phase 1: ë©€í‹°ë°ì´ íŠ¸ë Œë“œ ê°ì§€
# ============================================================

def get_latest_trading_date(max_days_back=7):
    """ìµœê·¼ ì˜ì—…ì¼ ë°˜í™˜."""
    today = datetime.now()
    for i in range(max_days_back):
        target = today - timedelta(days=i)
        date_str = target.strftime("%Y%m%d")
        try:
            tickers = pykrx_stock.get_market_ticker_list(date_str, market="KOSPI")
            if tickers and len(tickers) > 0:
                return date_str, target
        except:
            continue
    raise ValueError("ìµœê·¼ ì˜ì—…ì¼ ì—†ìŒ")


def fetch_multi_day_data(end_date_str, days=5):
    """Nì¼ OHLCV ë°ì´í„° ìˆ˜ì§‘."""
    end_date = datetime.strptime(end_date_str, "%Y%m%d")
    df_list = []
    current = end_date
    collected = 0

    while collected < days and (end_date - current).days < days * 3:
        date_str = current.strftime("%Y%m%d")
        try:
            daily_df = pykrx_stock.get_market_ohlcv_by_ticker(date_str, market="ALL")
            if daily_df is not None and len(daily_df) > 0:
                daily_df["date"] = date_str
                daily_df = daily_df[daily_df["ê±°ë˜ëŸ‰"] > 0]
                if len(daily_df) > 0:
                    df_list.append(daily_df)
                    collected += 1
        except:
            pass
        current -= timedelta(days=1)

    if not df_list:
        raise ValueError("ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")

    return pd.concat(df_list)


def calculate_trend_metrics(df):
    """íŠ¸ë Œë“œ ë©”íŠ¸ë¦­ ê³„ì‚°."""
    results = []
    for code in df.index.unique():
        stock_data = df.loc[[code]].sort_values("date")
        if len(stock_data) < 3:
            continue

        # ì¼ë³„ ë³€ë™ë¥ 
        changes = []
        prev_close = None
        for _, row in stock_data.iterrows():
            if prev_close:
                change = ((row["ì¢…ê°€"] - prev_close) / prev_close) * 100
                changes.append(change)
            prev_close = row["ì¢…ê°€"]

        if len(changes) < 2:
            continue

        # íŠ¸ë Œë“œ ê°ì§€
        consecutive_rise = all(c > 0 for c in changes[-3:]) if len(changes) >= 3 else False
        consecutive_fall = all(c < 0 for c in changes[-3:]) if len(changes) >= 3 else False

        recent_3 = changes[-3:] if len(changes) >= 3 else changes
        rise_count = sum(1 for c in recent_3 if c > 0)
        fall_count = sum(1 for c in recent_3 if c < 0)

        recent_volume = stock_data.iloc[-1]["ê±°ë˜ëŸ‰"]
        avg_volume = stock_data["ê±°ë˜ëŸ‰"].mean()
        volume_surge = (recent_volume / avg_volume) >= 1.5 if avg_volume > 0 else False

        # íŠ¸ë Œë“œ íƒ€ì… ê²°ì •
        trend_type = None
        trend_days = 0
        tier = None

        if consecutive_rise:
            trend_type, trend_days, tier = "consecutive_rise", len(changes), 1
        elif consecutive_fall:
            trend_type, trend_days, tier = "consecutive_fall", len(changes), 1
        elif rise_count >= 2:
            trend_type, trend_days, tier = "majority_rise", rise_count, 2
        elif fall_count >= 2:
            trend_type, trend_days, tier = "majority_fall", fall_count, 2
        elif volume_surge:
            trend_type, trend_days, tier = "volume_surge", 1, 3

        if trend_type:
            latest = stock_data.iloc[-1]
            results.append({
                "stock_code": code,
                "trend_type": trend_type,
                "trend_days": trend_days,
                "tier": tier,
                "close": int(latest["ì¢…ê°€"]),
                "change_rate": round(changes[-1], 2),
                "volume": int(latest["ê±°ë˜ëŸ‰"]),
            })

    return results


def select_top_trending(trending_stocks, target=15, indicators=None, macro_context=None):
    """ìƒìœ„ trending stocks ì„ íƒ.

    ê¸°ìˆ  ì§€í‘œ + ì„¹í„° ë¡œí…Œì´ì…˜ ê³ ë ¤ ì¢…í•© ì ìˆ˜ ê¸°ë°˜ ì„ ì •.
    indicators: {stock_code: {rsi, macd, macd_signal}} (optional)
    macro_context: {analysis, citations} (optional)
    """
    indicators = indicators or {}
    filtered = []

    # Phase 1: RSI í•„í„°ë§ (ê³¼ë§¤ìˆ˜ ê·¹ë‹¨ ì œì™¸)
    for s in trending_stocks:
        code = s["stock_code"]
        ind = indicators.get(code, {})
        rsi = ind.get("rsi")

        # RSI 80 ì´ìƒ (ê³¼ë§¤ìˆ˜ ê·¹ë‹¨) ì œì™¸, ìƒìŠ¹ íŠ¸ë Œë“œì¸ ê²½ìš°ë§Œ
        if rsi is not None and rsi > 80 and s.get("trend_type", "").endswith("rise"):
            continue
        # RSI ì •ë³´ë¥¼ ì¢…ëª©ì— ì¶”ê°€
        if rsi is not None:
            s["rsi"] = rsi
        if ind.get("macd_signal"):
            s["macd_signal"] = ind["macd_signal"]
        filtered.append(s)

    rsi_filtered = len(trending_stocks) - len(filtered)
    if rsi_filtered > 0:
        print(f"  RSI í•„í„°ë§: {rsi_filtered}ê°œ ê³¼ë§¤ìˆ˜ ì¢…ëª© ì œì™¸")

    # Phase 2: ê²½ê¸° ì‚¬ì´í´ë³„ ì„¹í„° ê°€ì¤‘ì¹˜
    if macro_context and macro_context.get("analysis"):
        cycle = determine_economic_cycle(macro_context)
        filtered = apply_sector_rotation_boost(filtered, cycle)
        print(f"  ê²½ê¸° ì‚¬ì´í´ íŒë‹¨: {cycle}")
    else:
        for s in filtered:
            s["rotation_boost"] = 1.0

    # Phase 3: ì¢…í•© ì ìˆ˜ ê³„ì‚°
    for s in filtered:
        base_score = (4 - s["tier"]) * 10  # Tier 1=30, Tier 2=20, Tier 3=10
        base_score += min(10, abs(s["change_rate"]))  # ë³€ë™ë¥  ë³´ë„ˆìŠ¤ (ìµœëŒ€ 10)

        # MACD ê³¨ë“ í¬ë¡œìŠ¤ ë³´ë„ˆìŠ¤
        if s.get("macd_signal") == "bullish":
            base_score += 5

        # Rotation ê°€ì¤‘ì¹˜
        base_score *= s.get("rotation_boost", 1.0)

        s["selection_score"] = round(base_score, 2)

    # ì ìˆ˜ ê¸°ë°˜ ì •ë ¬ ë° ì„ íƒ
    sorted_stocks = sorted(filtered, key=lambda x: x.get("selection_score", 0), reverse=True)
    return sorted_stocks[:target]


# ============================================================
# Phase 2: ì„¹í„° í´ëŸ¬ìŠ¤í„°ë§
# ============================================================

async def enrich_with_sectors(stocks):
    """stock_listingsì—ì„œ ì„¹í„° ì •ë³´ ì¡°íšŒ."""
    from app.core.database import AsyncSessionLocal
    from app.models.stock_listing import StockListing
    from sqlalchemy import select

    codes = [s["stock_code"] for s in stocks]

    async with AsyncSessionLocal() as session:
        result = await session.execute(select(StockListing).where(StockListing.stock_code.in_(codes)))
        listings = result.scalars().all()
        sector_map = {l.stock_code: l.sector for l in listings}

    for stock in stocks:
        stock["sector"] = sector_map.get(stock["stock_code"], "ê¸°íƒ€")

    return stocks


def cluster_by_sector(stocks):
    """ì„¹í„°ë³„ í´ëŸ¬ìŠ¤í„°ë§."""
    sector_groups = defaultdict(list)
    for s in stocks:
        sector_groups[s.get("sector", "ê¸°íƒ€")].append(s)

    themes = []
    trend_map = {
        "consecutive_rise": "ì—°ì† ìƒìŠ¹",
        "consecutive_fall": "ì—°ì† í•˜ë½",
        "majority_rise": "ìƒìŠ¹ ìš°ì„¸",
        "majority_fall": "í•˜ë½ ìš°ì„¸",
        "volume_surge": "ê±°ë˜ëŸ‰ ê¸‰ì¦",
    }

    for sector, sector_stocks in sector_groups.items():
        trend_groups = defaultdict(list)
        for s in sector_stocks:
            trend_groups[s["trend_type"]].append(s)

        for trend_type, trend_stocks in trend_groups.items():
            if len(trend_stocks) >= 2:
                # ì„¹í„° í…Œë§ˆ
                avg_change = sum(s["change_rate"] for s in trend_stocks) / len(trend_stocks)
                themes.append({
                    "type": "sector_theme",
                    "title": f"{sector} {trend_map.get(trend_type, 'ë³€ë™')} ì‹ í˜¸",
                    "sector": sector,
                    "stocks": trend_stocks,
                    "avg_change_rate": avg_change,
                    "stock_count": len(trend_stocks),
                })
            else:
                # ê°œë³„ ì¢…ëª©
                for s in trend_stocks:
                    themes.append({
                        "type": "individual_stock",
                        "title": f"{s.get('stock_name', s['stock_code'])} {trend_map.get(s['trend_type'], 'ë³€ë™')}",
                        "sector": sector,
                        "stocks": [s],
                        "avg_change_rate": s["change_rate"],
                        "stock_count": 1,
                    })

    return themes


def select_top_themes(themes, target=5):
    """ë‹¤ì–‘ì„± ê³ ë ¤ ìƒìœ„ í…Œë§ˆ ì„ íƒ."""
    sector_themes = [t for t in themes if t["type"] == "sector_theme"]
    individual = [t for t in themes if t["type"] == "individual_stock"]

    selected = []
    used_sectors = set()

    # ì„¹í„° í…Œë§ˆ ìš°ì„ 
    for t in sorted(sector_themes, key=lambda x: (x["stock_count"], abs(x["avg_change_rate"])), reverse=True):
        if t["sector"] not in used_sectors and len(selected) < target:
            selected.append(t)
            used_sectors.add(t["sector"])

    # ë¶€ì¡±í•˜ë©´ ê°œë³„ ì¢…ëª©
    for t in sorted(individual, key=lambda x: abs(x["avg_change_rate"]), reverse=True):
        if len(selected) < target:
            selected.append(t)

    return selected[:target]


# ============================================================
# Phase 3: RSS ë‰´ìŠ¤ ë§¤ì¹­
# ============================================================

def preprocess_news(news_list):
    """ë‰´ìŠ¤ ì „ì²˜ë¦¬: ì¤‘ë³µ ì œê±°."""
    if not news_list:
        return []

    unique = []
    seen = []

    for news in news_list:
        title = news["title"]
        is_dup = False
        for seen_title in seen:
            if SequenceMatcher(None, title, seen_title).ratio() >= 0.9:
                is_dup = True
                break
        if not is_dup:
            unique.append(news)
            seen.append(title)

    unique.sort(key=lambda x: x["published_at"], reverse=True)
    return unique


@retry_with_backoff(max_attempts=3, base_delay=2)
def match_news_to_stocks_llm(stocks, news, api_key):
    """LLM ë‰´ìŠ¤-ì¢…ëª© ë§¤ì¹­ (ì¬ì‹œë„ í¬í•¨)."""
    if not news or not stocks:
        return {}

    from openai import OpenAI

    client = OpenAI(api_key=api_key)
    stock_map = {s["stock_code"]: s.get("stock_name", s["stock_code"]) for s in stocks}
    result_map = {}

    # ë°°ì¹˜ ì²˜ë¦¬
    batch_size = 20
    for i in range(0, len(news), batch_size):
        batch = news[i : i + batch_size]
        news_str = "\n".join([f"{idx}. {n['title']}" for idx, n in enumerate(batch, 1)])
        stock_str = "\n".join([f"- {name} ({code})" for code, name in stock_map.items()])

        prompt = f"""ë‹¤ìŒ ë‰´ìŠ¤ë“¤ì´ ì–´ë–¤ ì¢…ëª©ì— ê´€í•œ ê²ƒì¸ì§€ íŒë‹¨í•˜ì„¸ìš”.

ë‰´ìŠ¤:
{news_str}

í›„ë³´ ì¢…ëª©:
{stock_str}

ì‘ë‹µ í˜•ì‹ (JSON):
{{"matches": [{{"news_index": 1, "stock_code": "005930"}}, {{"news_index": 2, "stock_code": "NONE"}}, ...]}}
"""

        try:
            resp = client.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": prompt}],
                response_format={"type": "json_object"},
                temperature=0,
            )
            data = json.loads(resp.choices[0].message.content)

            for match in data.get("matches", []):
                idx = match.get("news_index", 0) - 1
                code = match.get("stock_code", "NONE")
                if 0 <= idx < len(batch) and code != "NONE" and code in stock_map:
                    if code not in result_map:
                        result_map[code] = {
                            "title": batch[idx]["title"],
                            "url": batch[idx]["url"],
                            "published_at": batch[idx]["published_at"],
                            "source": batch[idx]["source"],
                        }
        except Exception as e:
            print(f"  âš ï¸  ë°°ì¹˜ {i // batch_size + 1} ë§¤ì¹­ ì‹¤íŒ¨: {e}")
            # ë°°ì¹˜ ì‹¤íŒ¨ëŠ” ì „ì²´ ì‹¤íŒ¨ë¡œ ì´ì–´ì§€ì§€ ì•ŠìŒ

    return result_map


# ============================================================
# Phase 4: í…Œë§ˆ í‚¤ì›Œë“œ ìƒì„±
# ============================================================

@retry_with_backoff(max_attempts=3, base_delay=2)
def generate_keyword_llm(theme, api_key):
    """LLM í‚¤ì›Œë“œ ìƒì„± (ì¬ì‹œë„ í¬í•¨)."""
    from openai import OpenAI

    client = OpenAI(api_key=api_key)

    sector = theme.get("sector", "ê¸°íƒ€")
    stocks = theme.get("stocks", [])
    avg_change = theme.get("avg_change_rate", 0)
    trend_type = stocks[0].get("trend_type", "") if stocks else ""
    trend_days = stocks[0].get("trend_days", 0) if stocks else 0

    hint = _SECTOR_MIRRORING_HINTS.get(sector, "ê³¼ê±° í•œêµ­ ì£¼ì‹ì‹œì¥ì˜ êµ¬ì²´ì  ì‚¬ë¡€")

    stock_names = [s.get("stock_name", s["stock_code"]) for s in stocks[:5]]
    stock_str = "Â·".join(stock_names)

    trend_desc = {"consecutive_rise": "ì—°ì† ìƒìŠ¹", "majority_rise": "ìƒìŠ¹ ìš°ì„¸"}.get(trend_type, "ë³€ë™")

    # ì„¹í„°/ë§¤í¬ë¡œ ë¶„ì„ ê²°ê³¼ (íŒŒì´í”„ë¼ì¸ì—ì„œ ì£¼ì…ë¨)
    sector_analysis = theme.get("sector_analysis", "")
    macro_context = theme.get("macro_context", "")

    extra_context = ""
    if sector_analysis:
        extra_context += f"\nì„¹í„° ì‹¬ì¸µ ë¶„ì„:\n{sector_analysis}\n"
    if macro_context:
        extra_context += f"\nê±°ì‹œê²½ì œ í™˜ê²½:\n{macro_context}\n"

    prompt = f"""í˜„ì¬ ì‹œì¥ ìƒí™©ì„ ë¶„ì„í•˜ì—¬ í‚¤ì›Œë“œë¥¼ ìƒì„±í•˜ì„¸ìš”.

í˜„ì¬:
- ì„¹í„°: {sector}
- ì¢…ëª©: {stock_str}
- íŠ¸ë Œë“œ: {trend_desc} ({trend_days}ì¼)
- ë³€ë™ë¥ : {avg_change:+.1f}%

ì—­ì‚¬ì  ì°¸ê³ :
{hint}
{extra_context}
ìš”êµ¬ì‚¬í•­:
1. 15ì ì´ë‚´ í‚¤ì›Œë“œ
2. 2-3ë¬¸ì¥ ì„¤ëª… (ì„¹í„° ë¶„ì„ê³¼ ê±°ì‹œê²½ì œ ë§¥ë½ì„ ë°˜ì˜)

ì‘ë‹µ (JSON):
{{"keyword": "í‚¤ì›Œë“œ", "description": "ì„¤ëª…"}}
"""

    try:
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.7,
        )
        data = json.loads(resp.choices[0].message.content)
        return {
            "title": data.get("keyword", theme.get("title", "")),
            "description": data.get("description", ""),
            "sector": sector,
            "stocks": [s["stock_code"] for s in stocks],
            "trend_days": trend_days,
            "trend_type": trend_type,
            "mirroring_hint": hint,
        }
    except Exception as e:
        print(f"  âš ï¸  í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨ (fallback): {e}")
        # Fallback: í…œí”Œë¦¿ ê¸°ë°˜
        return {
            "title": f"{sector} {trend_desc} ì‹ í˜¸",
            "description": f"{stock_str} {trend_days}ì¼ {trend_desc}",
            "sector": sector,
            "stocks": [s["stock_code"] for s in stocks],
            "trend_days": trend_days,
            "trend_type": trend_type,
            "mirroring_hint": hint,
        }


def calculate_quality_score(kw):
    """í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°."""
    score = 0
    if len(kw.get("stocks", [])) >= 2:
        score += 20
    if kw.get("sector"):
        score += 15
    if kw.get("mirroring_hint"):
        score += 15
    score += min(20, kw.get("trend_days", 0) * 5)
    if re.search(r"20\d{2}", kw.get("description", "")):
        score += 10
    return max(0, min(100, score))


# ============================================================
# í†µí•© íŒŒì´í”„ë¼ì¸
# ============================================================

async def run_integrated_pipeline():
    """Phase 1-4 í†µí•© ì‹¤í–‰ (ì˜ˆì™¸ ì²˜ë¦¬ í¬í•¨)."""
    print("=" * 70)
    print("ğŸš€ í†µí•© ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 70)

    openai_key = os.getenv("OPENAI_API_KEY")
    if not openai_key:
        print("âŒ OPENAI_API_KEY ì—†ìŒ")
        return False

    try:
        # Phase 1: ë©€í‹°ë°ì´ íŠ¸ë Œë“œ
        print("\n[Phase 1] ë©€í‹°ë°ì´ íŠ¸ë Œë“œ ê°ì§€")
        try:
            end_date_str, end_date_obj = get_latest_trading_date()
            print(f"  ìµœê·¼ ì˜ì—…ì¼: {end_date_str}")
        except Exception as e:
            print(f"âŒ ì˜ì—…ì¼ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False

        try:
            df_all = fetch_multi_day_data(end_date_str, days=5)
            print(f"  5ì¼ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(df_all)}ê±´")
        except Exception as e:
            print(f"âŒ ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return False

        trending = calculate_trend_metrics(df_all)
        print(f"  íŠ¸ë Œë“œ ê°ì§€: {len(trending)}ê°œ ì¢…ëª©")

        if len(trending) < 5:
            print(f"âš ï¸  íŠ¸ë Œë“œ ì¢…ëª© ë¶€ì¡± ({len(trending)}ê°œ), ìµœì†Œ 5ê°œ í•„ìš”")
            return False

        selected_stocks = select_top_trending(trending, target=15)
        print(f"  ìƒìœ„ {len(selected_stocks)}ê°œ ì„ íƒ ì™„ë£Œ")

        # ì¢…ëª©ëª… ì¶”ê°€
        for s in selected_stocks:
            try:
                s["stock_name"] = pykrx_stock.get_market_ticker_name(s["stock_code"])
            except:
                s["stock_name"] = s["stock_code"]

        # Phase 2: ì„¹í„° í´ëŸ¬ìŠ¤í„°ë§
        print("\n[Phase 2] ì„¹í„° í´ëŸ¬ìŠ¤í„°ë§")
        try:
            selected_stocks = await enrich_with_sectors(selected_stocks)
            print(f"  ì„¹í„° ì •ë³´ ë§¤í•‘ ì™„ë£Œ")
        except Exception as e:
            print(f"âš ï¸  ì„¹í„° ë§¤í•‘ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
            # ì„¹í„° ì •ë³´ ì—†ì–´ë„ ê³„ì† ì§„í–‰

        themes = cluster_by_sector(selected_stocks)
        print(f"  ìƒì„±ëœ í…Œë§ˆ: {len(themes)}ê°œ")

        selected_themes = select_top_themes(themes, target=5)
        print(f"  ìƒìœ„ {len(selected_themes)}ê°œ í…Œë§ˆ ì„ íƒ ì™„ë£Œ")

        # Phase 3: RSS ë‰´ìŠ¤ ë§¤ì¹­
        print("\n[Phase 3] RSS ë‰´ìŠ¤ ë§¤ì¹­")
        news_map = {}
        try:
            rss = RSSService(RSS_FEEDS)
            news = rss.fetch_top_news_structured()
            print(f"  RSS ë‰´ìŠ¤ ìˆ˜ì§‘: {len(news)}ê°œ")

            if news:
                news = preprocess_news(news)
                print(f"  ì „ì²˜ë¦¬ ì™„ë£Œ: {len(news)}ê°œ")

                news_map = match_news_to_stocks_llm(selected_stocks, news, openai_key)
                print(f"  ë‰´ìŠ¤-ì¢…ëª© ë§¤ì¹­: {len(news_map)}ê°œ")
            else:
                print("  âš ï¸  ë‰´ìŠ¤ ì—†ìŒ (ê³„ì† ì§„í–‰)")
        except Exception as e:
            print(f"âš ï¸  RSS ë‰´ìŠ¤ ë§¤ì¹­ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
            # ë‰´ìŠ¤ ë§¤ì¹­ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰

        # Phase 4: í‚¤ì›Œë“œ ìƒì„±
        print("\n[Phase 4] í…Œë§ˆ í‚¤ì›Œë“œ ìƒì„±")
        keywords = []
        for theme in selected_themes:
            try:
                kw = generate_keyword_llm(theme, openai_key)
                kw["quality_score"] = calculate_quality_score(kw)
                keywords.append(kw)
            except Exception as e:
                print(f"  âš ï¸  í…Œë§ˆ í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨: {e}")
                # ì‹¤íŒ¨í•œ í…Œë§ˆëŠ” ê±´ë„ˆëœ€

        if not keywords:
            print("âŒ í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨")
            return False

        print(f"  í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ: {len(keywords)}ê°œ")

        keywords_sorted = sorted(keywords, key=lambda k: k["quality_score"], reverse=True)
        final_keywords = keywords_sorted[:3]

        # ìµœì†Œ 3ê°œ ë³´ì¥
        if len(final_keywords) < 3:
            print(f"âš ï¸  í‚¤ì›Œë“œ {len(final_keywords)}ê°œë§Œ ìƒì„±ë¨, í…œí”Œë¦¿ ì¶”ê°€")
            # ê±°ë˜ëŸ‰ TOP ê°œë³„ ì¢…ëª©ìœ¼ë¡œ ë³´ì¶©
            for stock in sorted(selected_stocks, key=lambda s: s["volume"], reverse=True):
                if len(final_keywords) >= 3:
                    break
                fallback_kw = {
                    "title": f"{stock['stock_name']} ê±°ë˜ëŸ‰ ê¸‰ì¦",
                    "description": f"{stock['trend_days']}ì¼ íŠ¸ë Œë“œ, {stock['change_rate']:+.1f}%",
                    "sector": stock.get("sector", "ê¸°íƒ€"),
                    "stocks": [stock["stock_code"]],
                    "trend_days": stock["trend_days"],
                    "trend_type": stock["trend_type"],
                    "mirroring_hint": "",
                    "quality_score": 50,
                }
                final_keywords.append(fallback_kw)

        print(f"  ìµœì¢… ì„ íƒ: {len(final_keywords)}ê°œ í‚¤ì›Œë“œ")

        # DB ì €ì¥
        print("\n[ì €ì¥] DBì— ì €ì¥ ì¤‘...")
        try:
            await save_to_db(end_date_obj.date(), selected_stocks, news_map, final_keywords)
        except Exception as e:
            print(f"âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

        # Phase 5: Historical Cases ìë™ ìƒì„±
        print("\n[Phase 5] Historical Cases ìƒì„±")
        try:
            from scripts.generate_cases import main as generate_cases_main
            await generate_cases_main()
            print("  âœ… Historical cases ìƒì„± ì™„ë£Œ")
        except Exception as e:
            print(f"  âš ï¸  Historical cases ìƒì„± ì‹¤íŒ¨: {e}")
            print("  (í‚¤ì›Œë“œëŠ” ì •ìƒ ì €ì¥ë˜ì—ˆìœ¼ë‚˜, ê³¼ê±° ì‚¬ë¡€ ë§¤ì¹­ ë¯¸ì™„ë£Œ)")

        print("\n" + "=" * 70)
        print("âœ… í†µí•© íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print("=" * 70)
        return True

    except Exception as e:
        print(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return False


async def save_to_db(date, stocks, news_map, keywords):
    """DB ì €ì¥."""
    import asyncpg

    db_url = os.getenv("DATABASE_URL", "").replace("+asyncpg", "")
    if not db_url:
        db_url = "postgresql://narative:password@postgres:5432/narrative_invest"

    conn = await asyncpg.connect(db_url)

    # ì‹œì¥ ì§€ìˆ˜
    try:
        end_str = date.strftime("%Y%m%d")
        kospi = pykrx_stock.get_index_ohlcv(end_str, end_str, "1001")
        kosdaq = pykrx_stock.get_index_ohlcv(end_str, end_str, "2001")
        market_summary = f"KOSPI {kospi.iloc[0]['ì¢…ê°€']:.2f}, KOSDAQ {kosdaq.iloc[0]['ì¢…ê°€']:.2f}"
    except:
        market_summary = "ì‹œì¥ ì§€ìˆ˜ ì¡°íšŒ ì¤‘"

    # top_keywords êµ¬ì¡° ìƒì„±
    top_keywords = {"keywords": []}
    for kw in keywords:
        stock_codes = kw.get("stocks", [])
        stock_names = {s["stock_code"]: s["stock_name"] for s in stocks if s["stock_code"] in stock_codes}

        # news_mapì—ì„œ ì¹´íƒˆë¦¬ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ
        catalyst = None
        for code in stock_codes:
            if code in news_map:
                catalyst = news_map[code]
                break

        keyword_entry = {
            "title": kw.get("title", ""),
            "description": kw.get("description", ""),
            "category": kw.get("trend_type", ""),
            "tagline": kw.get("sector", ""),
            "sector": kw.get("sector", ""),  # ì§ì ‘ í•„ë“œ ì¶”ê°€
            "trend_days": kw.get("trend_days", 0),  # ì§ì ‘ í•„ë“œ ì¶”ê°€
            "trend_type": kw.get("trend_type", ""),  # ì§ì ‘ í•„ë“œ ì¶”ê°€
            "mirroring_hint": kw.get("mirroring_hint", ""),  # ì§ì ‘ í•„ë“œ ì¶”ê°€
            "catalyst": catalyst["title"] if catalyst else None,  # ì¹´íƒˆë¦¬ìŠ¤íŠ¸ ì¶”ê°€
            "stocks": [
                {
                    "stock_code": code,
                    "stock_name": stock_names.get(code, code),
                    "reason": f"{kw.get('trend_type', '')}, {kw.get('trend_days', 0)}ì¼ íŠ¸ë Œë“œ",
                }
                for code in stock_codes
            ],
            "quality_score": kw.get("quality_score", 0),
            "sources": {
                "market_data": {"provider": "pykrx", "date": date.isoformat(), "stocks": stock_codes},
                "sector_info": {"provider": "stock_listings", "sector": kw.get("sector", "")},
                "historical_hint": {"provider": "_SECTOR_MIRRORING_HINTS", "case": kw.get("mirroring_hint", "")},
                "news": [
                    {**catalyst, "citations": catalyst.get("citations", [])}
                    if catalyst else None
                    for catalyst in [news_map.get(code) for code in stock_codes]
                    if catalyst is not None
                ] or ([catalyst] if catalyst else []),  # citations í¬í•¨ ë‰´ìŠ¤ ì¶œì²˜
            },
        }
        top_keywords["keywords"].append(keyword_entry)

    # daily_briefings ì €ì¥ (idempotency: DELETE + INSERT)
    # ê°™ì€ ë‚ ì§œì˜ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ (briefing_stocks ë¨¼ì € ì‚­ì œ í›„ daily_briefings ì‚­ì œ)
    existing_id = await conn.fetchval(
        "SELECT id FROM daily_briefings WHERE briefing_date = $1",
        date
    )
    if existing_id:
        # briefing_stocks ë¨¼ì € ì‚­ì œ
        await conn.execute(
            "DELETE FROM briefing_stocks WHERE briefing_id = $1",
            existing_id
        )
        # daily_briefings ì‚­ì œ
        await conn.execute(
            "DELETE FROM daily_briefings WHERE id = $1",
            existing_id
        )
        print(f"  â†’ ê¸°ì¡´ ë°ì´í„° ì‚­ì œ: daily_briefings id={existing_id}")

    bid = await conn.fetchval(
        "INSERT INTO daily_briefings (briefing_date, market_summary, top_keywords, created_at) "
        "VALUES ($1, $2, $3::jsonb, NOW()) RETURNING id",
        date,
        market_summary,
        json.dumps(top_keywords, ensure_ascii=False),
    )
    print(f"  â†’ daily_briefings ì €ì¥: id={bid}")

    # briefing_stocks ì €ì¥
    rows = []
    for s in stocks:
        catalyst_info = news_map.get(s["stock_code"])
        catalyst_dt = None
        if catalyst_info:
            try:
                dt = datetime.fromisoformat(catalyst_info["published_at"])
                if dt.tzinfo is None:
                    dt = dt.replace(tzinfo=timezone.utc)
                catalyst_dt = dt
            except:
                pass

        rows.append((
            bid,
            s["stock_code"],
            s["stock_name"],
            s["change_rate"],
            s["volume"],
            s["trend_type"],
            datetime.now(),
            s["trend_days"],
            s["trend_type"],
            catalyst_info["title"] if catalyst_info else None,
            catalyst_info["url"] if catalyst_info else None,
            catalyst_dt,
            catalyst_info["source"] if catalyst_info else None,
        ))

    await conn.executemany(
        "INSERT INTO briefing_stocks "
        "(briefing_id, stock_code, stock_name, change_rate, volume, selection_reason, created_at, "
        "trend_days, trend_type, catalyst, catalyst_url, catalyst_published_at, catalyst_source) "
        "VALUES ($1,$2,$3,$4,$5,$6,$7,$8,$9,$10,$11,$12,$13)",
        rows,
    )
    print(f"  â†’ briefing_stocks ì €ì¥: {len(rows)}ê±´")

    await conn.close()


if __name__ == "__main__":
    asyncio.run(run_integrated_pipeline())
