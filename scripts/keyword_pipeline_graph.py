"""LangGraph ê¸°ë°˜ í‚¤ì›Œë“œ ìƒì„± íŒŒì´í”„ë¼ì¸.

ìˆœì°¨ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ë¥¼ LangGraph ë…¸ë“œ ê¸°ë°˜ìœ¼ë¡œ ì¬êµ¬ì„±í•˜ì—¬
- ê° ë‹¨ê³„ë³„ ì‹¤í–‰ ì‹œê°„ ì¶”ì 
- LangSmith ìë™ íŠ¸ë˜í‚¹
- ì—ëŸ¬ í•¸ë“¤ë§ ë° ì¬ì‹œë„
- ëª¨ë‹ˆí„°ë§ ì§€í‘œ ìˆ˜ì§‘
"""
import json
import os
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import Optional, TypedDict

from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langgraph.graph import END, START, StateGraph
from langsmith import traceable

# í•œêµ­ ê¸ˆìœµ ë‰´ìŠ¤ ë„ë©”ì¸ í•„í„° (Perplexity search_domain_filter)
KOREAN_FINANCIAL_DOMAINS = [
    "naver.com",
    "hankyung.com",
    "chosun.com",
    "mk.co.kr",
    "sedaily.com",
    "bloter.net",
    "etnews.com",
    "thebell.co.kr",
]

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ì¶”ê°€
project_root = Path(__file__).resolve().parent.parent
sys.path.insert(0, str(project_root))

# ê¸°ì¡´ í•¨ìˆ˜ë“¤ ì„í¬íŠ¸
from scripts.seed_fresh_data_integrated import (
    calculate_quality_score,
    calculate_technical_indicators,
    calculate_trend_metrics,
    cluster_by_sector,
    fetch_multi_day_data,
    get_latest_trading_date,
    select_top_themes,
    select_top_trending,
)


# ============================================================
# State ì •ì˜
# ============================================================


class KeywordPipelineState(TypedDict):
    """íŒŒì´í”„ë¼ì¸ ìƒíƒœ."""

    # Phase 1: Market data
    end_date_str: Optional[str]
    end_date_obj: Optional[datetime]
    raw_market_data: Optional[list]
    trending_stocks: Optional[list]

    # Phase 2: Sector clustering
    enriched_stocks: Optional[list]
    theme_clusters: Optional[list]
    selected_themes: Optional[list]

    # Phase 3: News matching (Perplexity catalysts)
    news_articles: Optional[list]
    stock_news_map: Optional[dict]

    # Phase 3-2: Sector/Macro analysis (Perplexity)
    sector_analyses: Optional[dict]  # sector â†’ {analysis, citations}
    macro_context: Optional[dict]  # {analysis, citations, timestamp}

    # Phase 4: Keyword generation
    keyword_candidates: Optional[list]
    final_keywords: Optional[list]

    # Metadata
    openai_api_key: str
    error: Optional[str]
    metrics: dict  # ì‹¤í–‰ ì‹œê°„, í† í° ì‚¬ìš©ëŸ‰ ë“±


# ============================================================
# Node í•¨ìˆ˜ë“¤
# ============================================================


@traceable(name="collect_market_data", run_type="tool")
def collect_market_data_node(state: KeywordPipelineState) -> dict:
    """Phase 1-1: pykrxë¡œ 5ì¼ ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘."""
    print("\n[Node] collect_market_data")

    if state.get("error"):
        return {}

    try:
        end_date_str, end_date_obj = get_latest_trading_date()
        print(f"  ìµœê·¼ ì˜ì—…ì¼: {end_date_str}")

        df_all = fetch_multi_day_data(end_date_str, days=5)
        # DataFrameì„ ê·¸ëŒ€ë¡œ ì „ë‹¬ (index êµ¬ì¡° ìœ ì§€ í•„ìš”)
        print(f"  5ì¼ ë°ì´í„° ìˆ˜ì§‘: {len(df_all)}ê±´")

        return {
            "end_date_str": end_date_str,
            "end_date_obj": end_date_obj,
            "raw_market_data": df_all,  # DataFrame ê·¸ëŒ€ë¡œ
        }
    except Exception as e:
        print(f"  âŒ ì‹œì¥ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return {"error": f"Market data collection failed: {e}"}


@traceable(name="filter_trends", run_type="tool")
def filter_trends_node(state: KeywordPipelineState) -> dict:
    """Phase 1-2: ë©€í‹°ë°ì´ íŠ¸ë Œë“œ í•„í„°ë§."""
    print("\n[Node] filter_trends")

    if state.get("error"):
        return {}

    try:
        # raw_market_dataëŠ” ì´ë¯¸ DataFrame
        df = state["raw_market_data"]
        end_date_str = state["end_date_str"]

        trending = calculate_trend_metrics(df)
        print(f"  íŠ¸ë Œë“œ ê°ì§€: {len(trending)}ê°œ ì¢…ëª©")

        if len(trending) < 5:
            return {"error": f"Too few trending stocks: {len(trending)}"}

        # RSI/MACD ê¸°ìˆ  ì§€í‘œ ê³„ì‚° (ìƒìœ„ í›„ë³´ ì¢…ëª©ë§Œ)
        top_codes = [s["stock_code"] for s in sorted(trending, key=lambda x: abs(x["change_rate"]), reverse=True)[:30]]
        indicators = {}
        try:
            indicators = calculate_technical_indicators(top_codes, end_date_str)
            print(f"  ê¸°ìˆ  ì§€í‘œ ê³„ì‚°: {len(indicators)}ê°œ ì¢…ëª© (RSI/MACD)")
        except Exception as e:
            print(f"  âš ï¸  ê¸°ìˆ  ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")

        selected = select_top_trending(trending, target=15, indicators=indicators)
        print(f"  ìƒìœ„ {len(selected)}ê°œ ì„ íƒ")

        # ì¢…ëª©ëª… ì¶”ê°€
        from pykrx import stock as pykrx_stock

        for s in selected:
            try:
                s["stock_name"] = pykrx_stock.get_market_ticker_name(s["stock_code"])
            except:
                s["stock_name"] = s["stock_code"]

        return {"trending_stocks": selected}
    except Exception as e:
        print(f"  âŒ íŠ¸ë Œë“œ í•„í„°ë§ ì‹¤íŒ¨: {e}")
        return {"error": f"Trend filtering failed: {e}"}


@traceable(name="enrich_sectors", run_type="tool")
def enrich_sectors_node(state: KeywordPipelineState) -> dict:
    """Phase 2-1: ì„¹í„° ì •ë³´ enrichment."""
    print("\n[Node] enrich_sectors")

    if state.get("error"):
        return {}

    try:
        import asyncio

        from scripts.seed_fresh_data_integrated import enrich_with_sectors

        stocks = state["trending_stocks"]
        enriched = asyncio.run(enrich_with_sectors(stocks))
        print(f"  ì„¹í„° ì •ë³´ ë§¤í•‘: {len(enriched)}ê°œ")

        return {"enriched_stocks": enriched}
    except Exception as e:
        print(f"  âš ï¸  ì„¹í„° ë§¤í•‘ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
        # ì„¹í„° ì •ë³´ ì—†ì–´ë„ ê³„ì† ì§„í–‰
        return {"enriched_stocks": state["trending_stocks"]}


@traceable(name="cluster_themes", run_type="tool")
def cluster_themes_node(state: KeywordPipelineState) -> dict:
    """Phase 2-2: ì„¹í„°ë³„ í…Œë§ˆ í´ëŸ¬ìŠ¤í„°ë§."""
    print("\n[Node] cluster_themes")

    if state.get("error"):
        return {}

    try:
        stocks = state["enriched_stocks"]
        themes = cluster_by_sector(stocks)
        print(f"  ìƒì„±ëœ í…Œë§ˆ: {len(themes)}ê°œ")

        selected = select_top_themes(themes, target=5)
        print(f"  ì„ íƒëœ í…Œë§ˆ: {len(selected)}ê°œ")

        return {"theme_clusters": themes, "selected_themes": selected}
    except Exception as e:
        print(f"  âŒ í…Œë§ˆ í´ëŸ¬ìŠ¤í„°ë§ ì‹¤íŒ¨: {e}")
        return {"error": f"Theme clustering failed: {e}"}


@traceable(name="search_catalysts_perplexity", run_type="llm")
def search_catalysts_perplexity_node(state: KeywordPipelineState) -> dict:
    """Phase 3: Perplexityë¡œ í…Œë§ˆë³„ ì¹´íƒˆë¦¬ìŠ¤íŠ¸ ë‰´ìŠ¤ ê²€ìƒ‰.

    - sonar-pro ëª¨ë¸ ì‚¬ìš© (ë” ë§ì€ citations, 200K context)
    - search_domain_filterë¡œ í•œêµ­ ê¸ˆìœµ ë‰´ìŠ¤ ë„ë©”ì¸ë§Œ ê²€ìƒ‰
    - citationsë¥¼ ê° articleì— ì €ì¥
    """
    print("\n[Node] search_catalysts_perplexity")

    if state.get("error"):
        return {}

    perplexity_key = os.getenv("PERPLEXITY_API_KEY", "")
    if not perplexity_key:
        print("  âš ï¸  PERPLEXITY_API_KEY ì—†ìŒ, ì¹´íƒˆë¦¬ìŠ¤íŠ¸ ìŠ¤í‚µ")
        return {"stock_news_map": {}, "news_articles": []}

    try:
        from openai import OpenAI

        client = OpenAI(api_key=perplexity_key, base_url="https://api.perplexity.ai")
        themes = state.get("selected_themes", [])
        stocks = state["enriched_stocks"]

        stock_news_map = {}  # stock_code â†’ {title, url, source, published_at, citations}
        all_articles = []

        for theme in themes:
            theme_stocks = theme.get("stocks", [])
            stock_names = [s.get("stock_name", s.get("stock_code", "")) for s in theme_stocks]
            sector = theme.get("sector", "")
            if not stock_names:
                continue

            query = (
                f"í•œêµ­ ì£¼ì‹ì‹œì¥ {sector} ì„¹í„° ìµœê·¼ 1ì£¼ì¼ ì£¼ìš” ë‰´ìŠ¤ë¥¼ ì•Œë ¤ì¤˜. "
                f"ê´€ë ¨ ì¢…ëª©: {', '.join(stock_names[:5])}. "
                f"ê° ì¢…ëª©ë³„ë¡œ ì£¼ê°€ì— ì˜í–¥ì„ ì¤€ í•µì‹¬ ë‰´ìŠ¤ 1ê°œì”©ë§Œ ì œëª©ê³¼ ì¶œì²˜ë¥¼ ì•Œë ¤ì¤˜. "
                f"JSON í˜•ì‹: [{{\"stock_name\": \"...\", \"title\": \"ë‰´ìŠ¤ ì œëª©\", \"source\": \"ì¶œì²˜ëª…\"}}]"
            )

            try:
                response = client.chat.completions.create(
                    model="sonar-pro",
                    messages=[{"role": "user", "content": query}],
                    search_domain_filter=KOREAN_FINANCIAL_DOMAINS,
                )

                content = response.choices[0].message.content
                citations = getattr(response, "citations", []) or []

                # JSON íŒŒì‹± ì‹œë„
                import re
                json_match = re.search(r'\[.*?\]', content, re.DOTALL)
                if json_match:
                    try:
                        news_items = json.loads(json_match.group())
                    except json.JSONDecodeError:
                        news_items = []
                else:
                    news_items = []

                # ì¢…ëª©ì½”ë“œì— ë§¤í•‘
                name_to_code = {s["stock_name"]: s["stock_code"] for s in stocks}
                for item in news_items:
                    sname = item.get("stock_name", "")
                    if sname in name_to_code:
                        code = name_to_code[sname]
                        catalyst = {
                            "title": item.get("title", ""),
                            "url": citations[0] if citations else "",
                            "source": item.get("source", "Perplexity"),
                            "published_at": datetime.now(timezone.utc).isoformat(),
                            "citations": citations,  # ì „ì²´ citations ì €ì¥
                        }
                        stock_news_map[code] = catalyst
                        all_articles.append({**catalyst, "stock_code": code, "stock_name": sname})

                # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì „ì²´ í…ìŠ¤íŠ¸ë¥¼ ì¹´íƒˆë¦¬ìŠ¤íŠ¸ë¡œ ì‚¬ìš©
                if not news_items and content.strip():
                    for sname in stock_names[:3]:
                        if sname in name_to_code:
                            code = name_to_code[sname]
                            if code not in stock_news_map:
                                catalyst = {
                                    "title": content[:200].strip(),
                                    "url": citations[0] if citations else "",
                                    "source": "Perplexity",
                                    "published_at": datetime.now(timezone.utc).isoformat(),
                                    "citations": citations,
                                }
                                stock_news_map[code] = catalyst
                                all_articles.append({**catalyst, "stock_code": code, "stock_name": sname})

            except Exception as e:
                print(f"  âš ï¸  í…Œë§ˆ '{sector}' Perplexity ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
                continue

        matched = len(stock_news_map)
        total = len(stocks)
        rate = (matched / total * 100) if total > 0 else 0
        print(f"  Perplexity ì¹´íƒˆë¦¬ìŠ¤íŠ¸ ë§¤ì¹­: {matched}/{total}ê°œ ({rate:.0f}%)")
        print(f"  citations ìˆ˜ì§‘: {sum(len(a.get('citations', [])) for a in all_articles)}ê°œ")

        return {"stock_news_map": stock_news_map, "news_articles": all_articles}

    except Exception as e:
        print(f"  âš ï¸  Perplexity ê²€ìƒ‰ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
        return {"stock_news_map": {}, "news_articles": []}


@traceable(name="research_sector_deep_dive", run_type="llm")
def research_sector_deep_dive_node(state: KeywordPipelineState) -> dict:
    """Phase 3-2: Perplexity ì„¹í„°ë³„ ì‹¬ì¸µ ë¶„ì„.

    ê° ì„ íƒëœ í…Œë§ˆì˜ ì„¹í„°ì— ëŒ€í•´ ê³µê¸‰ë§, ê²½ìŸ êµ¬ë„, ê·œì œ ë™í–¥ì„ ë¶„ì„í•œë‹¤.
    """
    print("\n[Node] research_sector_deep_dive")

    if state.get("error"):
        return {}

    perplexity_key = os.getenv("PERPLEXITY_API_KEY", "")
    if not perplexity_key:
        print("  âš ï¸  PERPLEXITY_API_KEY ì—†ìŒ, ì„¹í„° ë¶„ì„ ìŠ¤í‚µ")
        return {"sector_analyses": {}}

    try:
        from openai import OpenAI

        client = OpenAI(api_key=perplexity_key, base_url="https://api.perplexity.ai")
        themes = state.get("selected_themes", [])
        sector_analyses = {}
        analyzed_sectors = set()

        for theme in themes:
            sector = theme.get("sector", "")
            if not sector or sector in analyzed_sectors or sector == "ê¸°íƒ€":
                continue
            analyzed_sectors.add(sector)

            query = (
                f"í•œêµ­ ì£¼ì‹ì‹œì¥ {sector} ì„¹í„° ì‹¬ì¸µ ë¶„ì„:\n"
                f"1. ì£¼ìš” ê³µê¸‰ë§ êµ¬ì¡° (upstream/downstream í•µì‹¬ ê¸°ì—…)\n"
                f"2. ì£¼ìš” ê²½ìŸì‚¬ ë° ìµœê·¼ ì‹œì¥ ì ìœ ìœ¨ ë³€í™”\n"
                f"3. ìµœê·¼ ê·œì œ ë³€í™” ë° ì •ì±… ë™í–¥\n"
                f"4. í–¥í›„ 3-6ê°œì›” ì „ë§\n"
                f"ê°„ê²°í•˜ê²Œ ê° í•­ëª©ë³„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜."
            )

            try:
                response = client.chat.completions.create(
                    model="sonar-pro",
                    messages=[{"role": "user", "content": query}],
                    search_domain_filter=KOREAN_FINANCIAL_DOMAINS,
                )

                sector_analyses[sector] = {
                    "analysis": response.choices[0].message.content,
                    "citations": getattr(response, "citations", []) or [],
                }
                print(f"  ì„¹í„° '{sector}' ë¶„ì„ ì™„ë£Œ (citations: {len(sector_analyses[sector]['citations'])}ê°œ)")
            except Exception as e:
                print(f"  âš ï¸  ì„¹í„° '{sector}' ë¶„ì„ ì‹¤íŒ¨: {e}")
                continue

        print(f"  ì´ {len(sector_analyses)}ê°œ ì„¹í„° ë¶„ì„ ì™„ë£Œ")
        return {"sector_analyses": sector_analyses}

    except Exception as e:
        print(f"  âš ï¸  ì„¹í„° ë¶„ì„ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
        return {"sector_analyses": {}}


@traceable(name="research_macro_environment", run_type="llm")
def research_macro_environment_node(state: KeywordPipelineState) -> dict:
    """Phase 3-3: Perplexity ê±°ì‹œê²½ì œ í™˜ê²½ ë¶„ì„.

    ê¸°ì¤€ê¸ˆë¦¬, í™˜ìœ¨, ì‚°ì—… ì‚¬ì´í´, íˆ¬ìì ë™í–¥ì„ ë¶„ì„í•˜ì—¬
    í‚¤ì›Œë“œ í’ˆì§ˆ ì ìˆ˜ ì¡°ì • ë° ì„¹í„° ë¡œí…Œì´ì…˜ì— í™œìš©í•œë‹¤.
    """
    print("\n[Node] research_macro_environment")

    if state.get("error"):
        return {}

    perplexity_key = os.getenv("PERPLEXITY_API_KEY", "")
    if not perplexity_key:
        print("  âš ï¸  PERPLEXITY_API_KEY ì—†ìŒ, ë§¤í¬ë¡œ ë¶„ì„ ìŠ¤í‚µ")
        return {"macro_context": {}}

    try:
        from openai import OpenAI

        client = OpenAI(api_key=perplexity_key, base_url="https://api.perplexity.ai")

        query = (
            "í•œêµ­ ì£¼ì‹ì‹œì¥ ê±°ì‹œê²½ì œ í™˜ê²½ ë¶„ì„:\n"
            "1. í•œêµ­ì€í–‰ ê¸°ì¤€ê¸ˆë¦¬ í˜„í™© ë° í–¥í›„ 3ê°œì›” ì „ë§\n"
            "2. ì›/ë‹¬ëŸ¬ í™˜ìœ¨ ë™í–¥ ë° ì£¼ìš” ì˜í–¥ ìš”ì¸\n"
            "3. ë°˜ë„ì²´/ë°°í„°ë¦¬ ë“± ì£¼ë ¥ ì‚°ì—… ê²½ê¸° ì‚¬ì´í´ ë‹¨ê³„\n"
            "4. ì™¸êµ­ì¸/ê¸°ê´€ íˆ¬ìì ë™í–¥ (ìµœê·¼ 1ê°œì›”)\n"
            "ê°„ê²°í•˜ê²Œ ê° í•­ëª©ë³„ 2-3ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•´ì¤˜."
        )

        response = client.chat.completions.create(
            model="sonar-pro",
            messages=[{"role": "user", "content": query}],
            search_domain_filter=KOREAN_FINANCIAL_DOMAINS,
        )

        citations = getattr(response, "citations", []) or []
        macro_context = {
            "analysis": response.choices[0].message.content,
            "citations": citations,
            "timestamp": datetime.now(timezone.utc).isoformat(),
        }

        print(f"  ë§¤í¬ë¡œ ë¶„ì„ ì™„ë£Œ (citations: {len(citations)}ê°œ)")
        print(f"  ë¶„ì„ ê¸¸ì´: {len(macro_context['analysis'])}ì")
        return {"macro_context": macro_context}

    except Exception as e:
        print(f"  âš ï¸  ë§¤í¬ë¡œ ë¶„ì„ ì‹¤íŒ¨ (ê³„ì† ì§„í–‰): {e}")
        return {"macro_context": {}}


@traceable(name="generate_keywords", run_type="llm")
def generate_keywords_node(state: KeywordPipelineState) -> dict:
    """Phase 4-1: LLM í‚¤ì›Œë“œ ìƒì„±.

    sector_analysesì™€ macro_contextë¥¼ í™œìš©í•˜ì—¬ í‚¤ì›Œë“œ í’ˆì§ˆ í–¥ìƒ.
    """
    print("\n[Node] generate_keywords")

    if state.get("error"):
        return {}

    try:
        from scripts.seed_fresh_data_integrated import generate_keyword_llm

        themes = state["selected_themes"]
        api_key = state["openai_api_key"]
        sector_analyses = state.get("sector_analyses") or {}
        macro_context = state.get("macro_context") or {}
        keywords = []

        for theme in themes:
            try:
                # ì„¹í„° ë¶„ì„ ê²°ê³¼ë¥¼ í…Œë§ˆì— ì£¼ì…
                sector = theme.get("sector", "")
                if sector in sector_analyses:
                    theme["sector_analysis"] = sector_analyses[sector].get("analysis", "")[:500]
                if macro_context.get("analysis"):
                    theme["macro_context"] = macro_context["analysis"][:500]

                kw = generate_keyword_llm(theme, api_key)
                kw["quality_score"] = calculate_quality_score(kw)

                # ì„¹í„° ë¶„ì„ ì¡´ì¬ ì‹œ í’ˆì§ˆ ì ìˆ˜ ë³´ë„ˆìŠ¤
                if sector in sector_analyses:
                    kw["quality_score"] = min(100, kw["quality_score"] + 5)
                # ë§¤í¬ë¡œ ì»¨í…ìŠ¤íŠ¸ ì¡´ì¬ ì‹œ ì¶”ê°€ ë³´ë„ˆìŠ¤
                if macro_context.get("analysis"):
                    kw["quality_score"] = min(100, kw["quality_score"] + 5)

                keywords.append(kw)
            except Exception as e:
                print(f"  âš ï¸  í…Œë§ˆ í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨: {e}")

        print(f"  í‚¤ì›Œë“œ ìƒì„±: {len(keywords)}ê°œ")

        if not keywords:
            return {"error": "No keywords generated"}

        return {"keyword_candidates": keywords}
    except Exception as e:
        print(f"  âŒ í‚¤ì›Œë“œ ìƒì„± ì‹¤íŒ¨: {e}")
        return {"error": f"Keyword generation failed: {e}"}


@traceable(name="select_final_keywords", run_type="tool")
def select_final_keywords_node(state: KeywordPipelineState) -> dict:
    """Phase 4-2: í’ˆì§ˆ ì ìˆ˜ ê¸°ë°˜ ìµœì¢… í‚¤ì›Œë“œ ì„ íƒ."""
    print("\n[Node] select_final_keywords")

    if state.get("error"):
        return {}

    try:
        candidates = state["keyword_candidates"]
        stocks = state["enriched_stocks"]

        # ì ìˆ˜ìˆœ ì •ë ¬
        sorted_kw = sorted(candidates, key=lambda k: k["quality_score"], reverse=True)
        final = sorted_kw[:3]

        # ìµœì†Œ 3ê°œ ë³´ì¥ (fallback)
        if len(final) < 3:
            print(f"  âš ï¸  í‚¤ì›Œë“œ {len(final)}ê°œë§Œ ìƒì„±, í…œí”Œë¦¿ ì¶”ê°€")
            for stock in sorted(stocks, key=lambda s: s["volume"], reverse=True):
                if len(final) >= 3:
                    break
                fallback_kw = {
                    "title": f"{stock['stock_name']} ê±°ë˜ëŸ‰ ê¸‰ì¦",
                    "description": f"{stock['trend_days']}ì¼ íŠ¸ë Œë“œ, {stock['change_rate']:+.1f}%",
                    "sector": stock.get("sector", "ê¸°íƒ€"),
                    "stocks": [stock["stock_code"]],
                    "trend_days": stock["trend_days"],
                    "trend_type": stock["trend_type"],
                    "mirroring_hint": "",
                    "quality_score": 50,
                }
                final.append(fallback_kw)

        print(f"  ìµœì¢… ì„ íƒ: {len(final)}ê°œ í‚¤ì›Œë“œ")
        avg_score = sum(k["quality_score"] for k in final) / len(final)
        print(f"  í‰ê·  í’ˆì§ˆ ì ìˆ˜: {avg_score:.1f}/100")

        return {"final_keywords": final}
    except Exception as e:
        print(f"  âŒ ìµœì¢… ì„ íƒ ì‹¤íŒ¨: {e}")
        return {"error": f"Final selection failed: {e}"}


@traceable(name="save_to_database", run_type="tool")
def save_to_db_node(state: KeywordPipelineState) -> dict:
    """DB ì €ì¥."""
    print("\n[Node] save_to_database")

    if state.get("error"):
        return {}

    try:
        import asyncio

        from scripts.seed_fresh_data_integrated import save_to_db

        date = state["end_date_obj"].date()
        stocks = state["enriched_stocks"]
        news_map = state.get("stock_news_map", {})
        keywords = state["final_keywords"]

        asyncio.run(save_to_db(date, stocks, news_map, keywords))
        print("  âœ… DB ì €ì¥ ì™„ë£Œ")

        return {"metrics": {**state.get("metrics", {}), "db_saved": True}}
    except Exception as e:
        print(f"  âŒ DB ì €ì¥ ì‹¤íŒ¨: {e}")
        return {"error": f"DB save failed: {e}"}


# ============================================================
# ì¡°ê±´ë¶€ ë¼ìš°íŒ…
# ============================================================


def check_error(state: KeywordPipelineState) -> str:
    """ì—ëŸ¬ ë°œìƒ ì‹œ ENDë¡œ, ì•„ë‹ˆë©´ continue."""
    if state.get("error"):
        print(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì¤‘ë‹¨: {state['error']}")
        return "error"
    return "continue"


# ============================================================
# Graph ë¹Œë“œ
# ============================================================


def build_keyword_pipeline() -> StateGraph:
    """í‚¤ì›Œë“œ ìƒì„± íŒŒì´í”„ë¼ì¸ ê·¸ë˜í”„ ë¹Œë“œ."""
    graph = StateGraph(KeywordPipelineState)

    # ë…¸ë“œ ì¶”ê°€
    graph.add_node("collect_market_data", collect_market_data_node)
    graph.add_node("filter_trends", filter_trends_node)
    graph.add_node("enrich_sectors", enrich_sectors_node)
    graph.add_node("cluster_themes", cluster_themes_node)
    graph.add_node("search_catalysts", search_catalysts_perplexity_node)
    graph.add_node("research_sector", research_sector_deep_dive_node)
    graph.add_node("research_macro", research_macro_environment_node)
    graph.add_node("generate_keywords", generate_keywords_node)
    graph.add_node("select_final_keywords", select_final_keywords_node)
    graph.add_node("save_to_database", save_to_db_node)

    # ì—£ì§€ ì¶”ê°€
    graph.add_edge(START, "collect_market_data")

    graph.add_conditional_edges(
        "collect_market_data",
        check_error,
        {"error": END, "continue": "filter_trends"},
    )

    graph.add_conditional_edges(
        "filter_trends", check_error, {"error": END, "continue": "enrich_sectors"}
    )

    graph.add_edge("enrich_sectors", "cluster_themes")

    graph.add_conditional_edges(
        "cluster_themes", check_error, {"error": END, "continue": "search_catalysts"}
    )

    # search_catalysts â†’ 3ê°œ ë³‘ë ¬: research_sector, research_macro, generate_keywords ì¤€ë¹„
    # sector/macro ë¶„ì„ì€ ë³‘ë ¬ ì‹¤í–‰ í›„ generate_keywordsì—ì„œ í•©ë¥˜
    graph.add_edge("search_catalysts", "research_sector")
    graph.add_edge("search_catalysts", "research_macro")
    graph.add_edge("research_sector", "generate_keywords")
    graph.add_edge("research_macro", "generate_keywords")

    graph.add_conditional_edges(
        "generate_keywords",
        check_error,
        {"error": END, "continue": "select_final_keywords"},
    )

    graph.add_edge("select_final_keywords", "save_to_database")
    graph.add_edge("save_to_database", END)

    return graph.compile()


# ============================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================


@traceable(name="keyword_pipeline_full", run_type="chain")
def run_keyword_pipeline():
    """LangGraph í‚¤ì›Œë“œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰."""
    print("=" * 70)
    print("ğŸš€ LangGraph í‚¤ì›Œë“œ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 70)

    openai_key = os.getenv("OPENAI_API_KEY")
    if not openai_key:
        print("âŒ OPENAI_API_KEY ì—†ìŒ")
        return False

    # ì´ˆê¸° ìƒíƒœ
    initial_state = KeywordPipelineState(
        end_date_str=None,
        end_date_obj=None,
        raw_market_data=None,
        trending_stocks=None,
        enriched_stocks=None,
        theme_clusters=None,
        selected_themes=None,
        news_articles=None,
        stock_news_map=None,
        sector_analyses=None,
        macro_context=None,
        keyword_candidates=None,
        final_keywords=None,
        openai_api_key=openai_key,
        error=None,
        metrics={},
    )

    # ê·¸ë˜í”„ ë¹Œë“œ ë° ì‹¤í–‰
    pipeline = build_keyword_pipeline()

    try:
        start_time = datetime.now()
        result = pipeline.invoke(initial_state)
        end_time = datetime.now()

        elapsed = (end_time - start_time).total_seconds()
        print(f"\nâ±ï¸  ì´ ì‹¤í–‰ ì‹œê°„: {elapsed:.1f}ì´ˆ")

        if result.get("error"):
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {result['error']}")
            return False

        # ê²°ê³¼ ìš”ì•½
        print("\n" + "=" * 70)
        print("âœ… LangGraph íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
        print("=" * 70)
        print(f"ìµœì¢… í‚¤ì›Œë“œ: {len(result.get('final_keywords', []))}ê°œ")
        print(
            f"í‰ê·  í’ˆì§ˆ ì ìˆ˜: {sum(k['quality_score'] for k in result.get('final_keywords', [])) / len(result.get('final_keywords', [])) if result.get('final_keywords') else 0:.1f}/100"
        )
        print(f"íŠ¸ë Œë”© ì¢…ëª©: {len(result.get('enriched_stocks', []))}ê°œ")
        print(f"ë‰´ìŠ¤ ë§¤ì¹­: {len(result.get('stock_news_map', {}))}ê°œ")

        return True

    except Exception as e:
        print(f"\nâŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    import sys

    success = run_keyword_pipeline()
    sys.exit(0 if success else 1)
