# 안례진 — 프로젝트 기여 보고서 (STAR)

> 역할: AI QA (테스트, 프롬프트)
> 기간: 2026년 2월
> 기술 스택: pytest, Playwright, LangSmith, Perplexity API, LangChain

---

## Situation (상황)

- Adelie Investment 프로젝트는 AI 기반 콘텐츠 생성이 핵심인 만큼, LLM 출력의 품질 보증(환각 검증, 정확성 테스트)과 전체 시스템의 E2E 테스트가 필수적이었으나, 프로젝트 초기에는 테스트 인프라가 전무한 상태였다.
- PRD(Product Requirements Document) 수준의 요구사항 명세가 부족하여 구현 방향의 기준이 모호했으며, 데이터 수집 파이프라인의 소스 다각화도 필요한 상황이었다.
- AI 모델의 출력 품질을 체계적으로 검증하고, 서비스 안정성을 보장하기 위한 테스트 전략 수립이 시급하였다.
- LLM 에이전트의 동작을 추적하고 디버깅할 수 있는 관측 가능성(observability) 도구가 부재하였다.

## Task (과제)

- PRD Phase 0~1 요구사항 상세화 및 기능 명세 작성
- AI 환각 검증 테스트 프레임워크 설계 및 구현
- API 통합 테스트, 헬스체크, 스모크 테스트, E2E 테스트 케이스 작성
- Perplexity 기반 투자 사례 수집기 개발
- LangSmith 트레이싱 설정 및 AI 도구(용어집, 검색) 구현
- 프로젝트 문서화 및 데이터 파이프라인 가이드 작성

## Action (행동)

### 주요 구현 사항

#### 요구사항 정의 및 문서화

- **PRD Phase 0~1 요구사항 상세화 및 기능 명세 보완**: 프로젝트 초기 단계의 요구사항 문서를 상세화하였다.
  - Phase 0 (MVP) 핵심 기능 범위 정의: 키워드 조회, 내러티브 열람, AI 튜터 대화
  - Phase 1 확장 기능 명세: 포트폴리오, 리더보드, 학습 리포트
  - 각 기능의 인수 조건(acceptance criteria) 명시
  - 팀 전체가 구현 방향을 명확히 공유할 수 있도록 문서화
- **팀 문서 작성 및 테스트 인프라 구축**: 데이터 파이프라인 실행 가이드, 배포 가이드 등 팀 운영에 필요한 문서를 작성하였다.
  - `tests/` 디렉토리의 테스트 인프라(conftest.py, fixtures, 유틸리티) 구축
  - 테스트 실행 방법 가이드 작성

#### AI 품질 검증 테스트

- **AI 환각 검증 테스트 작성**: LLM이 생성하는 역사적 투자 사례와 내러티브 콘텐츠의 사실 정확성을 검증하는 테스트 프레임워크를 설계하였다.
  - 생성된 날짜의 유효성 검증 (실제 거래일 여부 확인)
  - 종목명 및 종목코드의 실존 여부 검증
  - 이벤트 내용의 팩트 체크 자동화
  - 환각(hallucination) 발생률 정량적 측정 체계 구축
- **시각화 모델 비교 테스트 추가**: 여러 LLM 모델의 차트 시각화 데이터 생성 품질을 비교하는 테스트를 작성하였다.
  - 비교 대상: GPT-4, Claude, Perplexity 등
  - 측정 항목: 생성 속도, 데이터 정확도, JSON 포맷 준수율
  - 모델별 비용 대비 품질 분석

#### 시스템 테스트

- **API 통합 테스트 케이스 작성**: FastAPI 백엔드의 주요 엔드포인트에 대한 통합 테스트 케이스를 작성하였다.
  - 테스트 대상: 키워드, 케이스, 인증, 용어집 엔드포인트
  - 정상 시나리오 및 에러 시나리오 커버
  - 인증 필요 엔드포인트의 권한 검증 테스트
- **API 헬스체크 및 스모크 테스트 추가**: 서비스 상태 모니터링 및 배포 후 검증 체계를 구축하였다.
  - 헬스체크 엔드포인트 테스트 (DB, Redis 연결 상태)
  - 배포 후 핵심 기능 정상 동작 확인 스모크 테스트
- **내러티브 및 튜터 E2E 플로우 테스트 추가**: 사용자의 전체 플로우를 테스트하는 E2E 테스트를 작성하였다.
  - 플로우: 랜딩 → 키워드 선택 → 내러티브 조회 → 튜터 질문
  - Playwright 활용 실제 브라우저 환경 테스트
  - 모바일 뷰포트(480px) 설정으로 모바일 퍼스트 환경 재현

#### 데이터 수집 및 AI 도구

- **Perplexity 기반 투자 사례 수집기 작성**: Perplexity API를 활용하여 과거 투자 사례를 자동으로 수집하는 수집기와 실행 스크립트를 개발하였다.
  - 웹 검색 기반의 사실적 데이터 확보
  - 수집 데이터 품질 검증 로직
  - 데이터베이스 적재 파이프라인
- **LangSmith 트레이싱 설정**: LangChain/LangGraph 실행의 추적 및 디버깅을 위해 LangSmith 트레이싱을 설정하였다.
  - AI 에이전트의 의사결정 과정 추적
  - 도구 호출 순서 모니터링
  - 토큰 사용량 및 비용 모니터링
- **용어집 및 검색 도구 구현**: AI 튜터 챗봇에서 사용하는 도구를 LangChain Tool로 구현하였다.
  - 금융 용어집 조회 도구
  - 관련 정보 검색 도구

### 기술적 의사결정

- **환각 검증 자동화 전략**: LLM 생성 콘텐츠의 환각을 수동이 아닌 자동 테스트로 검증하는 접근을 채택하였다. 검증 가능한 팩트(날짜, 종목코드, 수치)를 추출하여 외부 데이터 소스와 대조하는 방식을 설계하였다.
- **Playwright E2E 테스트 도입**: 프론트엔드-백엔드 통합 테스트에 Playwright를 도입하여 실제 브라우저 기반 테스트를 수행하였다. 모바일 뷰포트 설정을 포함하여 모바일 퍼스트 환경을 정확히 재현하였다.
- **Perplexity API 활용**: 투자 사례 수집에 Perplexity API를 활용하여 웹 검색 기반의 사실적 데이터를 확보하는 전략을 수립하였다. LLM 단독 생성 대비 사실 정확도가 높은 사례 데이터를 확보할 수 있었다.
- **계층별 테스트 구조**: `tests/unit/`, `tests/backend/`, `tests/integration/` 디렉토리 구조로 테스트를 분리하여, 단위 테스트 → 통합 테스트 → E2E 테스트 순서로 점진적 검증이 가능한 테스트 피라미드를 구성하였다.

## Result (결과)

### 정량적 성과

| 지표 | 수치 |
|------|------|
| 커밋 수 | 15개 |
| 코드 변경량 | +5,197 / -5 라인 |
| 활동일 수 | 5일 |
| 테스트 계층 | 3층 (unit / backend / integration) |

### 정성적 성과

- AI 환각 검증 테스트 프레임워크를 통해 LLM 생성 콘텐츠의 품질을 체계적으로 관리할 수 있는 기반을 마련하였다. 이는 금융 교육 플랫폼의 신뢰성 확보에 핵심적인 역할을 하였다.
- PRD 상세화를 통해 팀 전체의 구현 방향을 명확히 하여 불필요한 재작업을 줄이는 데 기여하였다.
- 다층 테스트 인프라 구축으로 배포 전 서비스 안정성을 검증할 수 있는 안전망을 확보하였다.
- Perplexity 기반 수집기를 통해 데이터 소스를 다각화하여 파이프라인의 콘텐츠 품질을 향상시켰다.
- LangSmith 트레이싱을 통해 AI 에이전트의 동작을 투명하게 모니터링할 수 있는 관측 가능성(observability)을 확보하였다.
